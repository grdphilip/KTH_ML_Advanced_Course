{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.special as sp_spec\n",
    "import scipy.stats as sp_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1A. Problem 1.4.19 SVI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data\n",
    "The cell below generates data for the LDA model. Note, for simplicity, we are using N_d = N for all d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(D, N, K, W, eta, alpha):\n",
    "    # sample K topics\n",
    "    beta = sp_stats.dirichlet(eta).rvs(size=K)  # size K x W\n",
    "\n",
    "    theta = np.zeros((D, K))   # size D x K\n",
    "\n",
    "    w = np.zeros((D, N, W))\n",
    "    z = np.zeros((D, N), dtype=int)\n",
    "    for d in range(D):\n",
    "        # sample document topic distribution\n",
    "        theta_d = sp_stats.dirichlet(alpha).rvs(size=1)\n",
    "        theta[d] = theta_d\n",
    "        for n in range(N):\n",
    "            # sample word to topic assignment\n",
    "            z_nd = sp_stats.multinomial(n=1, p=theta[d, :]).rvs(size=1).argmax(axis=1)[0]\n",
    "\n",
    "            # sample word\n",
    "            w_nd = sp_stats.multinomial(n=1, p=beta[z_nd, :]).rvs(1)\n",
    "\n",
    "            z[d, n] = z_nd\n",
    "            w[d, n] = w_nd\n",
    "\n",
    "    return w, z, theta, beta\n",
    "\n",
    "D_sim = 500\n",
    "N_sim = 50\n",
    "K_sim = 2\n",
    "W_sim = 5\n",
    "\n",
    "eta_sim = np.ones(W_sim)\n",
    "eta_sim[3] = 0.0001  # Expect word 3 to not appear in data\n",
    "eta_sim[1] = 3.  # Expect word 1 to be most common in data\n",
    "alpha_sim = np.ones(K_sim) * 1.0\n",
    "w0, z0, theta0, beta0 = generate_data(D_sim, N_sim, K_sim, W_sim, eta_sim, alpha_sim)\n",
    "w_cat = w0.argmax(axis=-1)  # remove one hot encoding\n",
    "unique_z, counts_z = numpy.unique(z0[0, :], return_counts=True)\n",
    "unique_w, counts_w = numpy.unique(w_cat[0, :], return_counts=True)\n",
    "\n",
    "# Sanity checks for data generation\n",
    "print(f\"Average z of each document should be close to theta of document. \\n Theta of doc 0: {theta0[0]} \\n Mean z of doc 0: {counts_z/N_sim}\")\n",
    "print(f\"Beta of topic 0: {beta0[0]}\")\n",
    "print(f\"Beta of topic 1: {beta0[1]}\")\n",
    "print(f\"Word to topic assignment, z, of document 0: {z0[0, 0:10]}\")\n",
    "print(f\"Observed words, w, of document 0: {w_cat[0, 0:10]}\")\n",
    "print(f\"Unique words and count of document 0: {[f'{u}: {c}' for u, c in zip(unique_w, counts_w)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributions as t_dist\n",
    "\n",
    "def generate_data_torch(D, N, K, W, eta, alpha):\n",
    "    \"\"\"\n",
    "    Torch implementation for generating data using the LDA model. Needed for sampling larger datasets.\n",
    "    \"\"\"\n",
    "    # sample K topics\n",
    "    beta_dist = t_dist.Dirichlet(torch.from_numpy(eta))\n",
    "    beta = beta_dist.sample([K])  # size K x W\n",
    "\n",
    "    # sample document topic distribution\n",
    "    theta_dist = t_dist.Dirichlet(torch.from_numpy(alpha))\n",
    "    theta = theta_dist.sample([D])\n",
    "\n",
    "    # sample word to topic assignment\n",
    "    z_dist = t_dist.OneHotCategorical(probs=theta)\n",
    "    z = z_dist.sample([N]).reshape(D, N, K)\n",
    "\n",
    "    # sample word from selected topics\n",
    "    beta_select = torch.einsum(\"kw, dnk -> dnw\", beta, z)\n",
    "    w_dist = t_dist.OneHotCategorical(probs=beta_select)\n",
    "    w = w_dist.sample([1])\n",
    "\n",
    "    w = w.reshape(D, N, W)\n",
    "\n",
    "    return w.numpy(), z.numpy(), theta.numpy(), beta.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_multivariate_beta_function(a, axis=None):\n",
    "    return np.sum(sp_spec.gammaln(a)) - sp_spec.gammaln(np.sum(a, axis=axis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAVI Implementation, ELBO and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_q(w, D, N, K, W):\n",
    "    \"\"\"\n",
    "    Random initialization.\n",
    "    \"\"\"\n",
    "    phi_init = np.random.random(size=(D, N, K))\n",
    "    phi_init = phi_init / np.sum(phi_init, axis=-1, keepdims=True)\n",
    "    gamma_init = np.random.randint(1, 10, size=(D, K))\n",
    "    lmbda_init = np.random.randint(1, 10, size=(K, W))\n",
    "    return phi_init, gamma_init, lmbda_init\n",
    "\n",
    "def update_q_Z(w, gamma, lmbda):\n",
    "    D, N, W = w.shape\n",
    "    K, W = lmbda.shape\n",
    "    E_log_theta = sp_spec.digamma(gamma) - sp_spec.digamma(np.sum(gamma, axis=1, keepdims=True))  # size D x K\n",
    "    E_log_beta = sp_spec.digamma(lmbda) - sp_spec.digamma(np.sum(lmbda, axis=1, keepdims=True))   # size K x W\n",
    "    log_rho = np.zeros((D, N, K))\n",
    "    w_label = w.argmax(axis=-1)\n",
    "    for d in range(D):\n",
    "        for n in range(N):\n",
    "            E_log_beta_wdn = E_log_beta[:, int(w_label[d, n])]\n",
    "            E_log_theta_d = E_log_theta[d]\n",
    "            log_rho_n = E_log_theta_d + E_log_beta_wdn\n",
    "            log_rho[d, n, :] = log_rho_n\n",
    "\n",
    "    phi = np.exp(log_rho - sp_spec.logsumexp(log_rho, axis=-1, keepdims=True))\n",
    "    return phi\n",
    "\n",
    "def update_q_theta(phi, alpha):\n",
    "    E_Z = phi\n",
    "    D, N, K = phi.shape\n",
    "    gamma = np.zeros((D, K))\n",
    "    for d in range(D):\n",
    "        E_Z_d = E_Z[d]\n",
    "        gamma[d] = alpha + np.sum(E_Z_d, axis=0)  # sum over N\n",
    "    return gamma\n",
    "\n",
    "def update_q_beta(w, phi, eta):\n",
    "    E_Z = phi\n",
    "    D, N, W = w.shape\n",
    "    K = phi.shape[-1]\n",
    "    lmbda = np.zeros((K, W))\n",
    "    for k in range(K):\n",
    "        lmbda[k, :] = eta\n",
    "        for d in range(D):\n",
    "            for n in range(N):\n",
    "                lmbda[k, :] += E_Z[d,n,k] * w[d,n]  # Sum over d and n\n",
    "    return lmbda\n",
    "\n",
    "def calculate_elbo(w, phi, gamma, lmbda, eta, alpha):\n",
    "    D, N, K = phi.shape\n",
    "    W = eta.shape[0]\n",
    "    E_log_theta = sp_spec.digamma(gamma) - sp_spec.digamma(np.sum(gamma, axis=1, keepdims=True))  # size D x K\n",
    "    E_log_beta = sp_spec.digamma(lmbda) - sp_spec.digamma(np.sum(lmbda, axis=1, keepdims=True))  # size K x W\n",
    "    E_Z = phi  # size D, N, K\n",
    "    log_Beta_alpha = log_multivariate_beta_function(alpha)\n",
    "    log_Beta_eta = log_multivariate_beta_function(eta)\n",
    "    log_Beta_gamma = np.array([log_multivariate_beta_function(gamma[d, :]) for d in range(D)])\n",
    "    dg_gamma = sp_spec.digamma(gamma)\n",
    "    log_Beta_lmbda = np.array([log_multivariate_beta_function(lmbda[k, :]) for k in range(K)])\n",
    "    dg_lmbda = sp_spec.digamma(lmbda)\n",
    "\n",
    "    neg_CE_likelihood = np.einsum(\"dnk, kw, dnw\", E_Z, E_log_beta, w)\n",
    "    neg_CE_Z = np.einsum(\"dnk, dk -> \", E_Z, E_log_theta)\n",
    "    neg_CE_theta = -D * log_Beta_alpha + np.einsum(\"k, dk ->\", alpha - 1, E_log_theta)\n",
    "    neg_CE_beta = -K * log_Beta_eta + np.einsum(\"w, kw ->\", eta - 1, E_log_beta)\n",
    "    H_Z = -np.einsum(\"dnk, dnk ->\", E_Z, np.log(E_Z))\n",
    "    gamma_0 = np.sum(gamma, axis=1)\n",
    "    dg_gamma0 = sp_spec.digamma(gamma_0)\n",
    "    H_theta = np.sum(log_Beta_gamma + (gamma_0 - K) * dg_gamma0 - np.einsum(\"dk, dk -> d\", gamma - 1, dg_gamma))\n",
    "    lmbda_0 = np.sum(lmbda, axis=1)\n",
    "    dg_lmbda0 = sp_spec.digamma(lmbda_0)\n",
    "    H_beta = np.sum(log_Beta_lmbda + (lmbda_0 - W) * dg_lmbda0 - np.einsum(\"kw, kw -> k\", lmbda - 1, dg_lmbda))\n",
    "    return neg_CE_likelihood + neg_CE_Z + neg_CE_theta + neg_CE_beta + H_Z + H_theta + H_beta\n",
    "\n",
    "def CAVI_algorithm(w, K, n_iter, eta, alpha):\n",
    "  D, N, W = w.shape\n",
    "  phi, gamma, lmbda = initialize_q(w, D, N, K, W)\n",
    "\n",
    "  # Store output per iteration\n",
    "  elbo = np.zeros(n_iter)\n",
    "  phi_out = np.zeros((n_iter, D, N, K))\n",
    "  gamma_out = np.zeros((n_iter, D, K))\n",
    "  lmbda_out = np.zeros((n_iter, K, W))\n",
    "\n",
    "  for i in range(0, n_iter):\n",
    "\n",
    "    ###### CAVI updates #######\n",
    "\n",
    "    # q(Z) update\n",
    "    phi = update_q_Z(w, gamma, lmbda)\n",
    "\n",
    "    # q(theta) update\n",
    "    gamma = update_q_theta(phi, alpha)\n",
    "\n",
    "    # q(beta) update\n",
    "    lmbda = update_q_beta(w, phi, eta)\n",
    "\n",
    "    # ELBO\n",
    "    elbo[i] = calculate_elbo(w, phi, gamma, lmbda, eta, alpha)\n",
    "\n",
    "    # outputs\n",
    "    phi_out[i] = phi\n",
    "    gamma_out[i] = gamma\n",
    "    lmbda_out[i] = lmbda\n",
    "\n",
    "  return phi_out, gamma_out, lmbda_out, elbo\n",
    "\n",
    "n_iter0 = 100\n",
    "K0 = K_sim\n",
    "W0 = W_sim\n",
    "eta_prior0 = np.ones(W0)\n",
    "alpha_prior0 = np.ones(K0)\n",
    "phi_out0, gamma_out0, lmbda_out0, elbo0 = CAVI_algorithm(w0, K0, n_iter0, eta_prior0, alpha_prior0)\n",
    "final_phi0 = phi_out0[-1]\n",
    "final_gamma0 = gamma_out0[-1]\n",
    "final_lmbda0 = lmbda_out0[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = 3\n",
    "print(f\"----- Recall label switching - compare E[theta] and true theta and check for label switching -----\")\n",
    "print(f\"Final E[theta] of doc 0 CAVI:  {np.round(final_gamma0[0] / np.sum(final_gamma0[0], axis=0, keepdims=True), precision)}\")\n",
    "print(f\"True theta of doc 0:          {np.round(theta0[0], precision)}\")\n",
    "\n",
    "print(f\"----- Recall label switching - e.g. E[beta_0] could be fit to true theta_1. -----\")\n",
    "print(f\"Final E[beta] k=0: {np.round(final_lmbda0[0, :] / np.sum(final_lmbda0[0, :], axis=-1, keepdims=True), precision)}\")\n",
    "print(f\"Final E[beta] k=1: {np.round(final_lmbda0[1, :] / np.sum(final_lmbda0[1, :], axis=-1, keepdims=True), precision)}\")\n",
    "print(f\"True beta k=0: {np.round(beta0[0, :], precision)}\")\n",
    "print(f\"True beta k=1: {np.round(beta0[1, :], precision)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVI Implementation\n",
    "\n",
    "Using the CAVI updates as a template, finish the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_Z_svi(batch, w, gamma, lmbda):\n",
    "    \"\"\"\n",
    "    TODO: rewrite to SVI update\n",
    "    \"\"\"\n",
    "    samples = w[batch]\n",
    "    D, N, W = samples.shape\n",
    "    K, W = lmbda.shape\n",
    "    E_log_theta = sp_spec.digamma(gamma) - sp_spec.digamma(np.sum(gamma, axis=1, keepdims=True))  # size D x K\n",
    "    E_log_beta = sp_spec.digamma(lmbda) - sp_spec.digamma(np.sum(lmbda, axis=1, keepdims=True))   # size K x W\n",
    "    log_rho = np.zeros((D, N, K))\n",
    "    w_label = samples.argmax(axis=-1)\n",
    "    for d in range(len(samples)):\n",
    "        for n in range(N):\n",
    "            E_log_beta_wdn = E_log_beta[:, int(w_label[d, n])]\n",
    "            E_log_theta_d = E_log_theta[d]\n",
    "            log_rho_n = E_log_theta_d + E_log_beta_wdn\n",
    "            log_rho[d, n, :] = log_rho_n\n",
    "\n",
    "    phi = np.exp(log_rho - sp_spec.logsumexp(log_rho, axis=-1, keepdims=True))\n",
    "    return phi\n",
    "\n",
    "def update_q_theta_svi(batch, phi, alpha):\n",
    "    \"\"\"\n",
    "    TODO: rewrite to SVI update\n",
    "    \"\"\"\n",
    "    E_Z = phi\n",
    "    D, N, K = phi.shape\n",
    "    gamma = np.ones((D, K))\n",
    "    for d in range(D):\n",
    "        E_Z_d = E_Z[d]\n",
    "        gamma[d] = alpha + np.sum(E_Z_d, axis=0)  # sum over N\n",
    "    return gamma\n",
    "\n",
    "def update_q_beta_svi(batch, w, phi, eta):\n",
    "    \"\"\"\n",
    "    TODO: rewrite to SVI update\n",
    "    \"\"\"\n",
    "    E_Z = phi\n",
    "    D, N, W = w.shape\n",
    "    K = phi.shape[-1]\n",
    "    lmbda = np.zeros((K, W))\n",
    "    for k in range(K):\n",
    "        lmbda[k, :] = eta\n",
    "        for d in range(D):\n",
    "            for n in range(N):\n",
    "                lmbda[k, :] += E_Z[d,n,k] * w[d,n]  # Sum over d and n\n",
    "    return lmbda\n",
    "\n",
    "def SVI_algorithm(w, K, S, n_iter, eta, alpha):\n",
    "  \"\"\"\n",
    "  Add SVI Specific code here.\n",
    "  \"\"\"\n",
    "  D, N, W = w.shape\n",
    "  phi, gamma, lmbda = initialize_q(w, D, N, K, W)\n",
    "\n",
    "  # Store output per iteration\n",
    "  elbo = np.zeros(n_iter)\n",
    "  phi_out = np.zeros((n_iter, D, N, K))\n",
    "  gamma_out = np.zeros((n_iter, D, K))\n",
    "  lmbda_out = np.zeros((n_iter, K, W))\n",
    "    \n",
    "  step_size = [1/(x**0.5) for x in range(1,n_iter+1)]\n",
    "  \n",
    "  for i in range(0, n_iter):\n",
    "    # Sample batch and set step size, rho.\n",
    "    batch = np.random.randint(0,D,S)\n",
    "    rho = step_size[i]\n",
    "    ###### SVI updates #######\n",
    "    \n",
    "    phi_prev = phi[batch].copy()\n",
    "    gamma_prev = gamma[batch].copy()\n",
    "    for j in range(20):\n",
    "        phi[batch] = update_q_Z_svi(batch,w,gamma_prev,lmbda)\n",
    "        gamma[batch] = update_q_theta_svi(batch, phi_prev, alpha)\n",
    "        \n",
    "        if (np.sum(np.abs(phi_prev - phi[batch])) < 0.1*S and (np.sum(np.abs(gamma_prev - gamma[batch])) < 0.1*S)):\n",
    "            break\n",
    "            \n",
    "        gamma_prev = gamma[batch].copy()\n",
    "        phi_prev = phi[batch].copy()\n",
    "\n",
    "    lmbda_intermediate = update_q_beta_svi(batch, w, phi, eta)\n",
    "    \n",
    "    lmbda_nxt = (1-rho)*lmbda + rho*lmbda_intermediate\n",
    "    \n",
    "    # ELBO\n",
    "    elbo[i] = calculate_elbo(w, phi, gamma, lmbda_nxt, eta, alpha)\n",
    "\n",
    "    # outputs\n",
    "    phi_out[i] = phi\n",
    "    gamma_out[i] = gamma\n",
    "    lmbda_out[i] = lmbda_nxt\n",
    "\n",
    "  return phi_out, gamma_out, lmbda_out, elbo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CASE 1\n",
    "Tiny dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# Data simulation parameters\n",
    "D1 = 50\n",
    "N1 = 50\n",
    "K1 = 2\n",
    "W1 = 5\n",
    "eta_sim1 = np.ones(W1)\n",
    "alpha_sim1 = np.ones(K1)\n",
    "\n",
    "w1, z1, theta1, beta1 = generate_data(D1, N1, K1, W1, eta_sim1, alpha_sim1)\n",
    "\n",
    "# Inference parameters\n",
    "n_iter_cavi1 = 100\n",
    "n_iter_svi1 = 100\n",
    "eta_prior1 = np.ones(W1) * 1.\n",
    "alpha_prior1 = np.ones(K1) * 1.\n",
    "S1 = 5 # batch size\n",
    "\n",
    "start_cavi1 = time.time()\n",
    "phi_out1_cavi, gamma_out1_cavi, lmbda_out1_cavi, elbo1_cavi = CAVI_algorithm(w1, K1, n_iter_cavi1, eta_prior1, alpha_prior1)\n",
    "end_cavi1 = time.time()\n",
    "\n",
    "start_svi1 = time.time()\n",
    "phi_out1_svi, gamma_out1_svi, lmbda_out1_svi, elbo1_svi = SVI_algorithm(w1, K1, S1, n_iter_svi1, eta_prior1, alpha_prior1)\n",
    "end_svi1 = time.time()\n",
    "\n",
    "final_phi1_cavi = phi_out1_cavi[-1]\n",
    "final_gamma1_cavi = gamma_out1_cavi[-1]\n",
    "final_lmbda1_cavi = lmbda_out1_cavi[-1]\n",
    "final_phi1_svi = phi_out1_svi[-1]\n",
    "final_gamma1_svi = gamma_out1_svi[-1]\n",
    "final_lmbda1_svi = lmbda_out1_svi[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "Do not expect perfect results in terms expectations being identical to the \"true\" theta and beta.\n",
    "Do not expect the ELBO plot of your SVI alg to be the same as the CAVI alg. However, it should increase and be in the same ball park as that of the CAVI alg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "print(f\"----- Recall label switching - compare E[theta] and true theta and check for label switching -----\")\n",
    "print(f\"E[theta] of doc 0 SVI:  {final_gamma1_svi[0] / np.sum(final_gamma1_svi[0], axis=0, keepdims=True)}\")\n",
    "print(f\"E[theta] of doc 0 CAVI: {final_gamma1_cavi[0] / np.sum(final_gamma1_cavi[0], axis=0, keepdims=True)}\")\n",
    "print(f\"True theta of doc 0:    {theta1[0]}\")\n",
    "\n",
    "print(f\"----- Recall label switching - e.g. E[beta_0] could be fit to true theta_1. -----\")\n",
    "print(f\"E[beta] SVI k=0:    {final_lmbda1_svi[0, :] / np.sum(final_lmbda1_svi[0, :], axis=-1, keepdims=True)}\")\n",
    "print(f\"E[beta] SVI k=1:    {final_lmbda1_svi[1, :] / np.sum(final_lmbda1_svi[1, :], axis=-1, keepdims=True)}\")\n",
    "print(f\"E[beta] CAVI k=0:   {final_lmbda1_cavi[0, :] / np.sum(final_lmbda1_cavi[0, :], axis=-1, keepdims=True)}\")\n",
    "print(f\"E[beta] CAVI k=1:   {final_lmbda1_cavi[1, :] / np.sum(final_lmbda1_cavi[1, :], axis=-1, keepdims=True)}\")\n",
    "print(f\"True beta k=0:      {beta1[0, :]}\")\n",
    "print(f\"True beta k=1:      {beta1[1, :]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(1, n_iter_cavi1 + 1)), elbo1_svi[np.arange(0, n_iter_svi1, int(n_iter_svi1 / n_iter_cavi1))], label=\"SVI\")\n",
    "plt.plot(list(range(1, n_iter_cavi1 + 1)), elbo1_cavi, label=\"cavi\")\n",
    "plt.title(\"ELBO plot\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"ELBO\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your own code for evaluation here (will not be graded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CASE 2\n",
    "Small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# Data simulation parameters\n",
    "D2 = 1000\n",
    "N2 = 50\n",
    "K2 = 3\n",
    "W2 = 10\n",
    "eta_sim2 = np.ones(W2)\n",
    "alpha_sim2 = np.ones(K2)\n",
    "\n",
    "w2, z2, theta2, beta2 = generate_data(D2, N2, K2, W2, eta_sim2, alpha_sim2)\n",
    "\n",
    "# Inference parameters\n",
    "n_iter_cavi2 = 100\n",
    "n_iter_svi2 = 100\n",
    "eta_prior2 = np.ones(W2) * 1.\n",
    "alpha_prior2 = np.ones(K2) * 1.\n",
    "S2 = 100 # batch size\n",
    "\n",
    "start_cavi2 = time.time()\n",
    "phi_out2_cavi, gamma_out2_cavi, lmbda_out2_cavi, elbo2_cavi = CAVI_algorithm(w2, K2, n_iter_cavi2, eta_prior2, alpha_prior2)\n",
    "end_cavi2 = time.time()\n",
    "\n",
    "start_svi2 = time.time()\n",
    "phi_out2_svi, gamma_out2_svi, lmbda_out2_svi, elbo2_svi = SVI_algorithm(w2, K2, S2, n_iter_svi2, eta_prior2, alpha_prior2)\n",
    "end_svi2 = time.time()\n",
    "\n",
    "final_phi2_cavi = phi_out2_cavi[-1]\n",
    "final_gamma2_cavi = gamma_out2_cavi[-1]\n",
    "final_lmbda2_cavi = lmbda_out2_cavi[-1]\n",
    "final_phi2_svi = phi_out2_svi[-1]\n",
    "final_gamma2_svi = gamma_out2_svi[-1]\n",
    "final_lmbda2_svi = lmbda_out2_svi[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "Do not expect perfect results in terms expectations being identical to the \"true\" theta and beta.\n",
    "Do not expect the ELBO plot of your SVI alg to be the same as the CAVI alg. However, it should increase and be in the same ball park as that of the CAVI alg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "print(f\"----- Recall label switching - compare E[theta] and true theta and check for label switching -----\")\n",
    "print(f\"E[theta] of doc 0 SVI:      {final_gamma2_svi[0] / np.sum(final_gamma2_svi[0], axis=0, keepdims=True)}\")\n",
    "print(f\"E[theta] of doc 0 CAVI:     {final_gamma2_cavi[0] / np.sum(final_gamma2_cavi[0], axis=0, keepdims=True)}\")\n",
    "print(f\"True theta of doc 0:        {theta2[0]}\")\n",
    "\n",
    "print(f\"----- Recall label switching - e.g. E[beta_0] could be fit to true theta_1. -----\")\n",
    "print(f\"E[beta] k=0:    {final_lmbda2_svi[0, :] / np.sum(final_lmbda2_svi[0, :], axis=-1, keepdims=True)}\")\n",
    "print(f\"E[beta] k=1:    {final_lmbda2_svi[1, :] / np.sum(final_lmbda2_svi[1, :], axis=-1, keepdims=True)}\")\n",
    "print(f\"E[beta] k=2:    {final_lmbda2_svi[2, :] / np.sum(final_lmbda2_svi[2, :], axis=-1, keepdims=True)}\")\n",
    "print(f\"True beta k=0:  {beta2[0, :]}\")\n",
    "print(f\"True beta k=1:  {beta2[1, :]}\")\n",
    "print(f\"True beta k=2:  {beta2[2, :]}\")\n",
    "\n",
    "print(f\"Time SVI: {end_svi2 - start_svi2}\")\n",
    "print(f\"Time CAVI: {end_cavi2 - start_cavi2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(1, n_iter_cavi2 + 1)), elbo2_svi[np.arange(0, n_iter_svi2, int(n_iter_svi2 / n_iter_cavi2))])\n",
    "plt.plot(list(range(1, n_iter_cavi2 + 1)), elbo2_cavi)\n",
    "plt.title(\"ELBO plot\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"ELBO\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your own code for evaluation here (will not be graded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CASE 3\n",
    "Medium small dataset, one iteration for time analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# Data simulation parameters\n",
    "D3 = 10**4\n",
    "N3 = 500\n",
    "K3 = 5\n",
    "W3 = 10\n",
    "eta_sim3 = np.ones(W3)\n",
    "alpha_sim3 = np.ones(K3)\n",
    "\n",
    "w3, z3, theta3, beta3 = generate_data_torch(D3, N3, K3, W3, eta_sim3, alpha_sim3)\n",
    "\n",
    "# Inference parameters\n",
    "n_iter3 = 1\n",
    "eta_prior3 = np.ones(W3) * 1.\n",
    "alpha_prior3 = np.ones(K3) * 1.\n",
    "S3 = 100 # batch size\n",
    "\n",
    "start_cavi3 = time.time()\n",
    "phi_out3_cavi, gamma_out3_cavi, lmbda_out3_cavi, elbo3_cavi = CAVI_algorithm(w3, K3, n_iter3, eta_prior3, alpha_prior3)\n",
    "end_cavi3 = time.time()\n",
    "\n",
    "start_svi3 = time.time()\n",
    "phi_out3_svi, gamma_out3_svi, lmbda_out3_svi, elbo3_svi = SVI_algorithm(w3, K3, S3, n_iter3, eta_prior3, alpha_prior3)\n",
    "end_svi3 = time.time()\n",
    "\n",
    "final_phi3_cavi = phi_out3_cavi[-1]\n",
    "final_gamma3_cavi = gamma_out3_cavi[-1]\n",
    "final_lmbda3_cavi = lmbda_out3_cavi[-1]\n",
    "final_phi3_svi = phi_out3_svi[-1]\n",
    "final_gamma3_svi = gamma_out3_svi[-1]\n",
    "final_lmbda3_svi = lmbda_out3_svi[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Examine per iteration run time.\")\n",
    "print(f\"Time SVI: {end_svi3 - start_svi3}\")\n",
    "print(f\"Time CAVI: {end_cavi3 - start_cavi3}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
